## 大文件上传

### 整体思路

- 前端：

利用 `Blob.prototype.slice` 方法，可以返回原文件的某个切片。

根据预先设置好的切片最大数量将文件切分为一个个切片，借助 http 的可并发性，同时上传多个切片，这样从原来传一个大文件，变成了同时传多个小的文件切片，减少上传时间。

由于是并发，传输到服务端的顺序可能发生变化，所以还需要给每个切片记录顺序。

- 后端：

何时合并? 一是前端在每个切片都携带切片的完整数量，当服务端收到这个数量的切片开始自动合并；二是额外发送一个请求主动通知服务端合并。

如何合并? 使用 nodejs 的读写流 (readStream/writeStream) 将所有的流传输到最终文件的流中。

### 简易代码

```html
<input type="file" onchange="handleFileChange" />
<button onclick="handleUpload">上传</button>
```

```javascript
let bigFile = null
// 选择文件
function handleFileChange(e) {
  const [file] = e.target.files
  if (!file) return
  bigFile = file
}
// 封装请求
function request({ url, method = "post", data, headers = {} }) {
  return new Promise((resolve) => {
    const xhr = new XMLHttpRequest()
    xhr.open(method, url)
    Object.keys(headers).forEach((key) => {
      xhr.setRequestHeader(key, headers[key])
    })
    xhr.send(data)
    xhr.onload = (e) => {
      resolve({
        data: e.target.response,
      })
    }
  })
}
// 上传切片
const SIZE = 10 * 1024 * 1024
const chunks = []
// 生成文件切片
function createFileChunk(file, size = SIZE) {
  const fileChunkList = []
  let cur = 0
  while (cur < file.size) {
    fileChunkList.push({ file: file.slice(cur, cur + size) })
    cur += size
  }
  return fileChunkList
}
// 点击上传
async function handleUpload() {
  if (!bigFile) return
  const fileChunkList = createFileChunk(bigFile)
  chunks = fileChunkList.map(function (item, index) {
    return {
      chunk: item.file,
      hash: bigFile.name + "-" + index, // 文件名+index 作为标识
    }
  })
  await uploadChunks()
}
// 上传切片
async function uploadChunks() {
  const requestList = chunks
    .map((item) => {
      const { chunk, hash } = item
      const formData = new FormData()
      formData.append("chunk", chunk)
      formData.append("hash", hash)
      formData.append("filename", bigFile.name)
      return { formData }
    })
    .map((data) => {
      request({
        url: "http://localhost:8080/upload",
        data: data.formData,
      })
    })
  await Promise.all(requestList)
  // 若采用主动通知服务端合并，额外发出请求
  await mergeRequest()
}
// 前端请求合并文件
function mergeRequest() {
  return request({
    url: "http://localhost:8080/merge",
    headers: { "Content-Type": "application/json" },
    data: JSON.stringify({ filename: bigFile.name }),
  })
}
```

### 切片进度条

XMLHttpRequest 原生支持上传进度的监听，为 xhr.upload.onprogress 添加回调函数。我们需要工厂函数为每个切片的上传都添加监听函数即可

```javascript
// 对封装请求进行修改
function request({
  url,
  method = "post",
  data,
  headers = {},
  onProgress = (e) => e, // +
}) {
  return new Promise((resolve) => {
    const xhr = new XMLHttpRequest()
    xhr.open(method, url)
    xhr.upload.onprogress = onProgress // +
    Object.keys(headers).forEach((key) => {
      xhr.setRequestHeader(key, headers[key])
    })
    xhr.send(data)
    xhr.onload = (e) => {
      resolve({
        data: e.target.response,
      })
    }
  })
}
// 为切片添加 percentage 记录上传进度
async function handleUpload() {
  if (!bigFile) return
  const fileChunkList = createFileChunk(bigFile)
  chunks = fileChunkList.map(function (item, index) {
    return {
      chunk: item.file,
      hash: bigFile.name + "-" + index, // 文件名+index 作为标识
      percentage: 0, // +
    }
  })
  await uploadChunks()
}
// 上传切片
async function uploadChunks() {
  const requestList = chunks
    .map((item) => {
      const { chunk, hash } = item
      const formData = new FormData()
      formData.append("chunk", chunk)
      formData.append("hash", hash)
      formData.append("filename", bigFile.name)
      return { formData, item } // +
    })
    .map((data) => {
      const { formData, item } = data
      request({
        url: "http://localhost:8080/upload",
        data: formData,
        onProgress: createProgressHandler(item), // +
      })
    })
  await Promise.all(requestList)
  await mergeRequest()
}
// 生成每个切片的进度监听事件
function createProgressHandler(item) {
  return (e) => (item.percentage = parseInt(String((e.loaded / e.total) * 100)))
}
// 计算总进度
function calcTotalPercentage() {
  if (!bigFile || !chunks.length) return 0
  const loaded = chunks
    .map((item) => item.chunk.size * item.percentage)
    .reduce((acc, cur) => acc + cur)
  return parseInt(String((loaded / bigFile.size) * 100))
}
```

## 断点续传

需要前端/后端记住已上传的切片，在下次上传时跳过已上传的部分

- 前端使用 localStorage 记录已上传的切片 hash；(切换浏览器失效)
- 后端保存已上传的切片 hash，前端在上传前像服务端获取已上传的切片

综合考虑服务端保存的方案更加合适，同时需解决几个问题：

### hash

之前使用的 `文件名 + index` 的 hash 在文件名修改后就失去了效果，而事实上只要文件内容不变，hash 就不应该变化。所以采用根据 `文件内容` 生成 hash。

这里用到另一个库 `spark-md5`，可以根据文件内容计算出文件的 hash 值，同时考虑到超大文件计算 hash 的耗时，可能会引发 `UI 的阻塞`，导致页面假死，所以采用 web-worker 在 worker 线程计算 hash。

web-worker 使用：

1. 实例化参数是一个 js 文件路径且不能跨域
2. worker 不允许访问 dom
3. 它提供的 importScripts 方法可以导入外部脚本，借此导入 spark-md5

```javascript
// public/hash,js
// 导入脚本
self.importScripts("./spark-md5.min.js")

// 生成 hash
self.onmessage = (e) => {
  const { fileChunkList } = e.data
  const spark = new self.SparkMD5.ArrayBuffer()
  let percentage = 0
  let count = 0
  const loadNext = (index) => {
    const reader = new FileReader()
    reader.readAsArrayBuffer(fileChunkList[index].file)
    reader.onload = (f) => {
      count++
      // spark-md5 需要对每一个切片进行计算
      // 直接使用整个文件可能不同文件也有相同的 hash
      spark.append(f.target.result)
      if (count === fileChunkList.length) {
        // 全部完成后将最终的 hash 发送给主线程
        self.postMessage({
          percentage: 100,
          hash: spark.end(),
        })
        self.close()
      } else {
        percentage += 100 / fileChunkList.length
        // 通过 postMessage 发送进度
        self.postMessage({
          percentage,
        })
        // 递归计算下一切片
        loadNext(count)
      }
    }
  }
  loadNext(0)
}
```

在主线程处理与 worker 线程的通信：

1. 使用 postMessage 传递 fileChunkList 切片
2. 监听 worker 的 postMessage 接收文件 hash

```javascript
// fileChunkList 的 hash 将通过 spark-md5 生成
const factory = {
  worker: null,
  hash: null,
  percentage: 0,
}
async function handleUpload() {
  if (!bigFile) return
  const fileChunkList = createFileChunk(bigFile)
  factory.hash = await calcFileHash(fileChunkList) // +
  chunks = fileChunkList.map(function (item, index) {
    return {
      chunk: item.file,
      hash: bigFile.name + "-" + index, // 文件名+index 作为标识
      fileHash: factory.hash, // +
      percentage: 0,
    }
  })
  await uploadChunks()
}
// 使用 web-worker 计算 hash
function calcFuleHash(fileChunkList) {
  return new Promise((resolve) => {
    factory.worker = new Worker("/hash.js")
    factory.worker.postMessage({ fileChunkList })
    factory.worker.onmessage = (e) => {
      const { percentage, hash } = e.data
      factory.percentage = percentage
      if (hash) {
        resolve(hash)
      }
    }
  })
}
```

> 文件秒传：服务器已经存在了上传的资源，所有当用户 `再次上传` 时直接提示上传成功。

文件秒传也就是依赖上面的文件 hash，在上传前计算出文件 hash，再通过服务器校验 hash 是否存在相同文件，存在直接返回上传成功的信息即可。

```javascript
// 校验 hash
function verifyUpload(filename, fileHash) {
  return request({
    url: 'http://localhost:8080/verify'
    headers: { "Content-Type": "application/json" },
    data: JSON.stringify({ filename, fileHash }),
  })
}
// 上传前校验
async function handleUpload() {
  if (!bigFile) return
  const fileChunkList = createFileChunk(bigFile)
  factory.hash = await calcFileHash(fileChunkList)
  const shouldUpload = await verifyUpload(bigFile.name, factory.hash)
  if (!shouldUpload) {
    // 提示秒传成功
    return
  }
  chunks = fileChunkList.map(function (item, index) {
    return {
      chunk: item.file,
      hash: bigFile.name + "-" + index, // 文件名+index 作为标识
      fileHash: factory.hash,
      percentage: 0,
    }
  })
  await uploadChunks()
}
```

### 暂停

断点续传在断点 (暂停) 时，使用 XMLHttpRequest 的 abort 方法，取消 xhr 请求的发送，所以我们需要将 xhr 对象保存起来，以备暂停使用

```javascript
let xhrList = []
// 对封装请求进行修改
function request({
  url,
  method = "post",
  data,
  headers = {},
  onProgress = (e) => e,
  requestList, // +
}) {
  return new Promise((resolve) => {
    const xhr = new XMLHttpRequest()
    xhr.open(method, url)
    xhr.upload.onprogress = onProgress
    Object.keys(headers).forEach((key) => {
      xhr.setRequestHeader(key, headers[key])
    })
    xhr.send(data)
    xhr.onload = (e) => {
      // 将请求成功的 xhr 从列表删除
      if (requestList) {
        // +
        const xhrIndex = requestList.findIndex((item) => item === xhr) // +
        requestList.splice(xhrIndex, 1) // +
      } // +
      resolve({
        data: e.target.response,
      })
    }
    // 将 xhr 保存到 requestList
    requestList?.push(xhr)
  })
}
// 手动暂停
function handlePause() {
  xhrList.forEach((xhr) => xhr?.abort())
  xhrList = []
}
```

### 续传

续传除了秒传的情况，在服务器不存在该文件或已上传部分文件切片时，通知前端上传，并把 `已上传` 的文件切片返回给前端。

```javascript
// 恢复前查询已上传的切片
async function handleResume() {
  const { uploadedList } = await verifyUpload(bigFile.name, factory.hash)
  await uploadChunks(uploadedList)
}
// 上传前校验也进行修改
async function handleUpload() {
  if (!bigFile) return
  const fileChunkList = createFileChunk(bigFile)
  factory.hash = await calcFileHash(fileChunkList)
  const { shouldUpload, uploadedList } = await verifyUpload(
    bigFile.name,
    factory.hash
  ) // +
  if (!shouldUpload) {
    // 提示秒传成功
    return
  }
  chunks = fileChunkList.map(function (item, index) {
    return {
      chunk: item.file,
      hash: bigFile.name + "-" + index, // 文件名+index 作为标识
      fileHash: factory.hash,
      percentage: 0,
    }
  })
  await uploadChunks(uploadedList) // +
}
// 上传切片时过滤已上传的部分
async function uploadChunks(uploadedList) {
  const requestList = chunks
    .filter((item) => !uploadedList.includes(item.hash)) // +
    .map((item) => {
      const { chunk, hash } = item
      const formData = new FormData()
      formData.append("chunk", chunk)
      formData.append("hash", hash)
      formData.append("filename", bigFile.name)
      return { formData, item }
    })
    .map((data) => {
      const { formData, item } = data
      request({
        url: "http://localhost:8080/upload",
        data: formData,
        onProgress: createProgressHandler(item),
        requestList: xhrList, // +
      })
    })
  await Promise.all(requestList)
  // 之前上传的切片数量 + 本次上传的切片数量 = 所有切片数量
  if (uploadedList.length + requestList.length === chunks.length) {
    // 合并切片
    await mergeRequest()
  } // +
}
```

## 后续优化

1. 进度条因为暂停恢复出现 `倒退` 的问题
2. 切片上传失败的处理
3. 需要回显时，自动获取上传切片
