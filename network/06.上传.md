## 大文件上传

### 整体思路

- 前端：

利用 `Blob.prototype.slice` 方法，可以返回原文件的某个切片。

根据预先设置好的切片最大数量将文件切分为一个个切片，借助 http 的可并发性，同时上传多个切片，这样从原来传一个大文件，变成了同时传多个小的文件切片，减少上传时间。

由于是并发，传输到服务端的顺序可能发生变化，所以还需要给每个切片记录顺序。

- 后端：

何时合并? 一是前端在每个切片都携带切片的完整数量，当服务端收到这个数量的切片开始自动合并；二是额外发送一个请求主动通知服务端合并。

如何合并? 使用 nodejs 的读写流 (readStream/writeStream) 将所有的流传输到最终文件的流中。

### 简易代码

```html
<input type="file" onchange="handleFileChange" />
<button onclick="handleUpload">上传</button>
```

```javascript
let bigFile = null
// 选择文件
function handleFileChange(e) {
  const [file] = e.target.files
  if (!file) return
  bigFile = file
}
// 封装请求
function request({ url, method = "post", data, headers = {} }) {
  return new Promise((resolve) => {
    const xhr = new XMLHttpRequest()
    xhr.open(method, url)
    Object.keys(headers).forEach((key) => {
      xhr.setRequestHeader(key, headers[key])
    })
    xhr.send(data)
    xhr.onload = (e) => {
      resolve({
        data: e.target.response,
      })
    }
  })
}
// 上传切片
const SIZE = 10 * 1024 * 1024
const chunks = []
// 生成文件切片
function createFileChunk(file, size = SIZE) {
  const fileChunkList = []
  let cur = 0
  while (cur < file.size) {
    fileChunkList.push({ file: file.slice(cur, cur + size) })
    cur += size
  }
  return fileChunkList
}
// 点击上传
async function handleUpload() {
  if (!bigFile) return
  const fileChunkList = createFileChunk(bigFile)
  chunks = fileChunkList.map(function (item, index) {
    return {
      chunk: item.file,
      hash: bigFile.name + "-" + index, // 文件名+index 作为标识
    }
  })
  await uploadChunks()
}
// 上传切片
async function uploadChunks() {
  const requestList = chunks
    .map((item) => {
      const { chunk, hash } = item
      const formData = new FormData()
      formData.append("chunk", chunk)
      formData.append("hash", hash)
      formData.append("filename", bigFile.name)
      return { formData }
    })
    .map((data) => {
      request({
        url: "http://localhost:8080/upload",
        data: data.formData,
      })
    })
  await Promise.all(requestList)
  // 若采用主动通知服务端合并，额外发出请求
  await mergeRequest()
}
// 前端请求合并文件
function mergeRequest() {
  return request({
    url: "http://localhost:8080/merge",
    headers: { "Content-Type": "application/json" },
    data: JSON.stringify({ filename: bigFile.name }),
  })
}
```

## 切片进度条

XMLHttpRequest 原生支持上传进度的监听，为 xhr.upload.onprogress 添加回调函数。我们需要工厂函数为每个切片的上传都添加监听函数即可

```javascript
// 对封装请求进行修改
function request({
  url,
  method = "post",
  data,
  headers = {},
  onProgress = (e) => e, // +
}) {
  return new Promise((resolve) => {
    const xhr = new XMLHttpRequest()
    xhr.open(method, url)
    xhr.upload.onprogress = onProgress // +
    Object.keys(headers).forEach((key) => {
      xhr.setRequestHeader(key, headers[key])
    })
    xhr.send(data)
    xhr.onload = (e) => {
      resolve({
        data: e.target.response,
      })
    }
  })
}
// 为切片添加 percentage 记录上传进度
async function handleUpload() {
  if (!bigFile) return
  const fileChunkList = createFileChunk(bigFile)
  chunks = fileChunkList.map(function (item, index) {
    return {
      chunk: item.file,
      hash: bigFile.name + "-" + index, // 文件名+index 作为标识
      percentage: 0, // +
    }
  })
  await uploadChunks()
}
// 上传切片
async function uploadChunks() {
  const requestList = chunks
    .map((item) => {
      const { chunk, hash } = item
      const formData = new FormData()
      formData.append("chunk", chunk)
      formData.append("hash", hash)
      formData.append("filename", bigFile.name)
      return { formData, item } // +
    })
    .map((data) => {
      const { formData, item } = data
      request({
        url: "http://localhost:8080/upload",
        data: formData,
        onProgress: createProgressHandler(item),  // +
      })
    })
  await Promise.all(requestList)
  await mergeRequest()
}
// 生成每个切片的进度监听事件
function createProgressHandler(item) {
  return (e) => (item.percentage = parseInt(String((e.loaded / e.total) * 100)))
}
// 计算总进度
function calcTotalPercentage() {
  if (!bigFile || !chunks.length) return 0
  const loaded = chunks
    .map((item) => item.chunk.size * item.percentage)
    .reduce((acc, cur) => acc + cur)
  return parseInt(String((loaded / bigFile.size) * 100))
}
```
